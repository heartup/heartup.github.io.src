---
layout: post
categories: math
---

### Univariate 线性回归

Hypothesis：\\(h_{\\theta} = \\theta _0 + \\theta _1 x\\)

Parameters: \\(\\theta _0, \\theta _1\\)

Cost: \\(J(\\theta \_0, \\theta \_1)=\\frac{1}{2m}\\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)})^2\\)

Goal: 找到使\\(J(\\theta \_0, \\theta \_1)\\)最小的\\(\\theta \_0, \\theta \_1\\)

#### 梯度下降算法

repeat until convergence {

\\(\\theta _j := \\theta _j - \\alpha \\frac{\\partial}{\\partial \\theta _j} J(\\theta \_0, \\theta \_1)\\)  (for j=0 and j=1)

}

其中\\(\\frac{\\partial}{\\partial \\theta _j} J(\\theta \_0, \\theta \_1) = \\frac{1}{m} \\sum\_{i=1}^m (h\_\\theta (x^{(i)}) - y^{(i)}) x^{(i)} _j\\)

在这里，假定\\(x^{(i)} _0 = 1, x^{(i)} _1 = x\\)，当做了这种假定时，可以将Univariate线性回归推广到多元线性回归。

这种梯度下降算法成为Batch Gradient Descent，因为每一步计算用到了训练集里面的所有数据。
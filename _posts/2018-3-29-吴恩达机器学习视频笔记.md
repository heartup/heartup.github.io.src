---
layout: post
categories: math
---

### Univariate 线性回归

Hypothesis：\\(h_{\\theta} = \\theta _0 + \\theta _1 x\\)

Parameters: \\(\\theta _0, \\theta _1\\)

Cost: \\(J(\\theta \_0, \\theta \_1)=\\frac{1}{2m}\\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)})^2\\), 其中\\(x^{(i)}\\)表示第\\(i\\)个样本点的输入，
\\(y^{(i)}\\)表示第\\(i\\)个样本点的应有的输出值, \\(m\\)表示训练集中样本点的个数

Goal: 找到使\\(J(\\theta \_0, \\theta \_1)\\)最小的\\(\\theta \_0, \\theta \_1\\)


#### 梯度下降算法

repeat until convergence {

\\(\\theta _j := \\theta _j - \\alpha \\frac{\\partial}{\\partial \\theta _j} J(\\theta \_0, \\theta \_1)\\)  (for j=0 and j=1)

}

其中\\(\\frac{\\partial}{\\partial \\theta _j} J(\\theta \_0, \\theta \_1) = \\frac{1}{m} \\sum\_{i=1}^m (h\_\\theta (x^{(i)}) - y^{(i)}) x^{(i)} _j\\)

在这里，假定\\(x^{(i)} _0 = 1, x^{(i)} _1 = x\\)，当做了这种假定时，可以将Univariate线性回归推广到多元线性回归。

这种梯度下降算法成为Batch Gradient Descent，因为每一步计算用到了训练集里面的所有数据。


#### 多项式回归

举例说明，如果假设函数是如下的形式：

Hypothesis：\\(h_{\\theta} = \\theta _0 + \\theta _1 x + \\theta _2 x^2 + \\theta _3 x^3\\)

那么做下面的转换，就可以将这种假设函数转化成多元线性回归的假设函数，类似上面的假设函数的问题称为多项式回归问题。这种转化即：

\\(y_1 = x, y_2 = x^2, y_3 = x^3\\)


#### 逻辑回归

逻辑回归用于解决分类问题，对于2分类问题（分类结果为0或者1），我们在线性回归的模型（假设函数）外层，增加一个sigmoid函数，使函数值映射在(0, 1)区间，
我们将这个得到的函数值解释为将x映射到\\(y = 1\\)的概率，即\\(P(y=1 | x;\\theta)\\)。逻辑回归的假设函数的表示形式如下：

Hypothesis：\\(h_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^T x}}\\)

对于逻辑回归的Cost函数，如果用和线性回归同样的方式，那么它就是non-convex函数，这种函数求最小值通常是很难的。因此使用如下形式的Cost函数：

Cost：\\(J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m Cost(h_\\theta (x^{(i)}),y^{(i)})\\)

其中：\\(Cost(h_\\theta (x^{(i)}),y^{(i)}) = -y \\ln {(h_\\theta (x))} - (1 - y)\\ln {(1 - {h_\\theta (x)})}\\) 注意其中y只可能取0，或者1

使用这种形式的Cost函数，梯度下降算法将和线性回归一致（因为计算导数的公式一致）。
---
layout: post
categories: math
---

### Univariate 线性回归

Hypothesis：\\(h_{\\theta} = \\theta _0 + \\theta _1 x\\)

Parameters: \\(\\theta _0, \\theta _1\\)

Cost: \\(J(\\theta \_0, \\theta \_1)=\\frac{1}{2m}\\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)})^2\\), 其中\\(x^{(i)}\\)表示第\\(i\\)个样本点的输入，
\\(y^{(i)}\\)表示第\\(i\\)个样本点的应有的输出值, \\(m\\)表示训练集中样本点的个数

Goal: 找到使\\(J(\\theta \_0, \\theta \_1)\\)最小的\\(\\theta \_0, \\theta \_1\\)


#### 梯度下降算法

repeat until convergence {

\\(\\theta _j := \\theta _j - \\alpha \\frac{\\partial}{\\partial \\theta _j} J(\\theta \_0, \\theta \_1)\\)  (for j=0 and j=1)

}

其中\\(\\frac{\\partial}{\\partial \\theta _j} J(\\theta \_0, \\theta \_1) = \\frac{1}{m} \\sum\_{i=1}^m (h\_\\theta (x^{(i)}) - y^{(i)}) x^{(i)} _j\\)

在这里，假定\\(x^{(i)} _0 = 1, x^{(i)} _1 = x\\)，当做了这种假定时，可以将Univariate线性回归推广到多元线性回归。

这种梯度下降算法成为Batch Gradient Descent，因为每一步计算用到了训练集里面的所有数据。


#### 多项式回归

举例说明，如果假设函数是如下的形式：

Hypothesis：\\(h_{\\theta} = \\theta _0 + \\theta _1 x + \\theta _2 x^2 + \\theta _3 x^3\\)

那么做下面的转换，就可以将这种假设函数转化成多元线性回归的假设函数，类似上面的假设函数的问题称为多项式回归问题。这种转化即：

\\(y_1 = x, y_2 = x^2, y_3 = x^3\\)


#### 逻辑回归（2元分类问题）

逻辑回归用于解决分类问题，对于2分类问题（分类结果为0或者1），我们在线性回归的模型（假设函数）外层，增加一个sigmoid函数，使函数值映射在(0, 1)区间，
我们将这个得到的函数值解释为将x映射到\\(y = 1\\)的概率，即\\(P(y=1 | x;\\theta)\\)。逻辑回归的假设函数的表示形式如下：

Hypothesis：\\(h_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^T x}}\\)

对于逻辑回归的Cost函数，如果用和线性回归同样的方式，那么它就是non-convex函数，这种函数求最小值通常是很难的。因此使用如下形式的Cost函数：

Cost：\\(J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m Cost(h_\\theta (x^{(i)}),y^{(i)})\\)

其中：\\(Cost(h_\\theta (x^{(i)}),y^{(i)}) = -y \\ln {(h_\\theta (x))} - (1 - y)\\ln {(1 - {h_\\theta (x)})}\\) 注意其中y只可能取0，或者1

使用这种形式的Cost函数，梯度下降算法将和线性回归一致（因为计算导数的公式一致）。


#### 多元分类问题

多元分类问题可以分解为多个2元（1对其余）分类问题。1对其余的方法陈述如下：

为每个分类\\(i\\)构造一个逻辑分类器\\(h_\\theta ^{(i)} (x)\\)。这个\\(h\\)的意义为\\(y = i\\)的概率。
即：\\(h_\\theta ^{(i)} (x) = P(y = i |x;\\theta) (i = 1, 2, 3)\\)。

当有一个新的输入\\(x\\)，用以下方法做出预测：选择分类\\(i\\)使得\\(\\max h_\\theta ^{(i)} (x)\\)。

对于简单的情况，特征输入不是很多的情况，可以用以上的分类方法做出预测，但是，如果输入特征非常多（比如图像的识别，每个像素是一个特征，另外如果要达到一些非线性的效果，还要增加这些特征值的幂的组合），
以上的分类方法的计算量太大导致上面的方法不再适用。下面介绍的神经网络可以用于这类问题的分类。


#### 深度神经网络可以解决复杂问题的直观解释

And, Or可以用2分类方法单层神经网络（线性回归）的方式解决，考虑对于XOR的问题怎么解决，观察到如下关系：

$$A \oplus B = (\lnot (A \land B)) \lor ((\lnot A) \land (\lnot B))$$

可以通过多层神经元（将前一层的输出作为后一层的输入值）的方式解决XOR操作的问题。这也解释了单个神经元为什么具有记忆的功能。


#### 神经网络的COST函数


---
layout: post
categories: math
---

#### 示例神经网络

![nn.png](https://heartup.github.io/static/img/nn.jpg)

训练集包含100条数据，每条数据有3个特征值，对应的输出有3个值（分3类），底标均从1开始。

#### 标志说明

\\(x^{(i)}_j\\)表示训练集第\\(i\\)条数据的第\\(j\\)个特征值，同时表示矩阵\\(X\\)的第\\(i\\)行第\\(j\\)列，

\\(y^{(i)}_j\\)表示训练集第\\(i\\)条数据应有的输出向量的第\\(j\\)个值，同时表示矩阵\\(Y\\)的第\\(i\\)行第\\(j\\)列，

\\(w^{(i \\to j)} _{m n}\\)表示第\\(i\\)层的第\\(m\\)个神经元到第\\(j\\)层的第\\(n\\)个神经元之间的连线，同时表示矩阵\\(W^{(i \\to j)}\\)的第\\(m\\)行第\\(n\\)列，

\\(zl^{(i)} _j \\)表示训练集第\\(i\\)条数据第\\(l\\)层第\\(j\\)个神经元中激活函数的输入，

\\(al^{(i)} _j \\)表示训练集第\\(i\\)条数据第\\(l\\)层第\\(j\\)个神经元中激活函数的输出，同时也是矩阵\\(Al\\)的第\\(i\\)行第\\(j\\)列，

\\(cost^{(i)}\\)表示训练集第\\(i\\)条数据的损失函数，其中

$$ cost^{(i)} = \sum _j cost^{(i)}_j, cost^{(i)}_j = \frac{1}{2} (a3 ^{(i)} _j - y ^{(i)} _j )^2$$

\\(J\\)表示整个训练集对应的损失函数，其中

$$ J = \frac{1}{100} \sum _{i = 1} ^{100} cost^{(i)} $$

#### 计算过程

+ 计算\\(\\epsilon 3 = A3 - Y\\)，即

$$\epsilon 3 = \begin{bmatrix}
{a3^{(1)}_1 - y^{(1)}_1}&{a3^{(1)}_2 - y^{(1)}_2}&{a3^{(1)}_3 - y^{(1)}_3}\\
{a3^{(2)}_1 - y^{(2)}_1}&{a3^{(2)}_2 - y^{(2)}_2}&{a3^{(2)}_3 - y^{(2)}_3}\\
{\vdots}&{\vdots}&{\vdots}\\
{a3^{(100)}_1 - y^{(100)}_1}&{a3^{(100)}_2 - y^{(100)}_2}&{a3^{(100)}_3 - y^{(100)}_3}\\
\end{bmatrix} $$

容易知道

$$\epsilon 3_{ij} = \frac{\partial{cost^{(i)}_j}}{\partial{a3^{(i)}_j}} $$

+ 计算\\(\\delta 3 = \\epsilon 3 .* g'(A3)\\)，其中\\(g' _{ij} = a3^{(i)} _j (1 - a3^{(i)} _j)\\)

容易知道

$$\delta 3_{ij} = \frac{\partial{cost^{(i)}_j}}{\partial{z3^{(i)}_j}} $$

+ 计算\\(\\epsilon 2 = \\delta 3 \\times (W^{(2\\to 3)})^T \\)

注意到\\(\\delta 3 \\)是\\(100\\times3\\)的矩阵，\\(W^{(2\\to 3)}\\)是\\(4\\times3\\)的矩阵，故\\(\\epsilon 2\\)是\\(100\\times4\\)的矩阵。

观察到

$$\epsilon 2_{45,3} = \delta 3 ^{(45)} _1 w^{(2 \to 3)}_{3,1} + \delta 3 ^{(45)} _2 w^{(2 \to 3)}_{3,2} + \delta 3 ^{(45)} _3 w^{(2 \to 3)}_{3,3}$$

根据链式法则知道

$$\epsilon 2 _{i,j} = \frac{\partial cost^{(i)}}{\partial a2^{(i)}_j}$$

+ 计算\\(\\delta 2 = \\epsilon 2 .* g'(A2)\\)，其中\\(g' _{ij} = a2^{(i)} _j (1 - a2^{(i)} _j)\\)

容易知道

$$\delta 2_{ij} = \frac{\partial{cost^{(i)}}}{\partial{z2^{(i)}_j}} $$

+ 计算\\(\\varphi ^{(2 \to 3)} = A2^T \\times \\delta 3\\)

注意到\\(\\delta 3 \\)是\\(100\\times3\\)的矩阵，\\(A2\\)是\\(100\\times 4\\)的矩阵，故\\(\\varphi ^{(2 \to 3)} \\)是\\(4\\times3\\)的矩阵，
并且与\\(W ^{(2 \to 3)} \\)的形式相同。

观察到

$$\varphi ^{(2 \to 3)} _{3,2} = a2^{(1)} _3 \delta 3 _{1,2} + a2^{(2)} _3 \delta 3 _{2,2} + a2^{(3)} _3 \delta 3 _{3,2} + \cdots + a2^{(100)} _3 \delta 3 _{100,2} $$

因为

$$\frac{\partial cost^{(i)}}{w^{(2 \to 3)} _{3,2}} = \frac{\partial cost^{(i)} _2}{w^{(2 \to 3)} _{3,2}} $$

$$\frac{\partial z3^{(i)} _2}{w^{(2 \to 3)} _{3,2}} = a2^{(i)} _3$$

结合

$$\delta 3_{i,2} = \frac{\partial{cost^{(i)}_2}}{\partial{z3^{(i)}_2}} $$

知道

$$a2^{(i)} _3 \delta 3_{i,2} = \frac{\partial cost^{(i)}}{w^{(2 \to 3)} _{3,2}}$$

因此

$$\varphi ^{(2 \to 3)} _{3,2} = 100 \times \frac{\partial J}{w^{(2 \to 3)} _{3,2}} $$

至此，我们知道可以用公式

$$W^{(2 \to 3)} = W^{(2 \to 3)} - \alpha \varphi ^{(2 \to 3)}$$

优化\\(W^{(2 \\to 3)}\\)

+ 计算\\(\\varphi ^{(1 \to 2)} = A1^T \\times \\delta 2\\)

注意到\\(\\delta 2 \\)是\\(100\\times 4\\)的矩阵，\\(A1\\)是\\(100\\times 3\\)的矩阵，故\\(\\varphi ^{(1 \to 2)} \\)是\\(3\\times4\\)的矩阵，
并且与\\(W ^{(1 \to 2)} \\)的形式相同。

观察到

$$\varphi ^{(1 \to 2)} _{2,3} = a1^{(1)} _2 \delta 2 _{1,3} + a1^{(2)} _2 \delta 2 _{2,3} + a1^{(3)} _2 \delta 2 _{3,3} + \cdots + a1^{(100)} _2 \delta 2 _{100,3} $$

因为

$$\frac{\partial z2^{(i)} _3}{w^{(1 \to 2)} _{2,3}} = a1^{(i)} _2$$

结合

$$\delta 2_{i,3} = \frac{\partial{cost^{(i)}}}{\partial{z2^{(i)}_3}} $$

知道

$$a1^{(i)} _2 \delta 2_{i,3} = \frac{\partial cost^{(i)}}{w^{(1 \to 2)} _{2,3}}$$

因此

$$\varphi ^{(1 \to 2)} _{2,3} = 100 \times \frac{\partial J}{w^{(1 \to 2)} _{2,3}} $$

至此，我们知道可以用公式

$$W^{(1 \to 2)} = W^{(1 \to 2)} - \alpha \varphi ^{(1 \to 2)}$$

优化\\(W^{(1 \\to 2)}\\)
